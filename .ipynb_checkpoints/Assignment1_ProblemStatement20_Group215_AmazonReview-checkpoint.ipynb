{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR0oAITvK3_E"
   },
   "source": [
    "# NLP (AIMLCZG530) - Assignment 1 - Problem Statement 20 - Group 215\n",
    "---\n",
    "## Team Members & Contribution\n",
    "\n",
    "| Student ID | Student Name | Contribution % |\n",
    "|---|---|---|\n",
    "| 2024aa05899 | Bhavin Arvindkumar Shah | 20% |\n",
    "| xx | xx | 20% |\n",
    "| xx | xx | 20% |\n",
    "| xx | xx | 20% |\n",
    "| xx | xx | 20% |\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Choosen - Amazon Product Review\n",
    "**Justification:**\n",
    "We have chosen the Amazon Product Review dataset for its rich linguistic diversity and relevance to real-world natural language processing tasks. This dataset contains a wide range of product reviews written by various users, reflecting different writing styles, sentiments, and domain-specific vocabulary. This diversity makes it an excellent testbed for developing robust Part-of-Speech (POS) tagging models.\n",
    "\n",
    "**Dataset source:**\n",
    "https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_q8JHYmxQ531"
   },
   "source": [
    "#### Download & import necessary Libraries & NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-yYMEGL-QoyU",
    "outputId": "23202b73-adc4-4142-ba1e-449c2546318b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.3)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp39-cp39-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: click in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2024.2)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (56.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.73.0-cp39-cp39-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: namex in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (3.16.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.19.0-cp39-cp39-win_amd64.whl (375.7 MB)\n",
      "   ---------------------------------------- 0.0/375.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.7/375.7 MB 19.8 MB/s eta 0:00:19\n",
      "    --------------------------------------- 6.3/375.7 MB 15.5 MB/s eta 0:00:24\n",
      "    --------------------------------------- 9.2/375.7 MB 14.6 MB/s eta 0:00:26\n",
      "   - -------------------------------------- 11.5/375.7 MB 14.1 MB/s eta 0:00:26\n",
      "   - -------------------------------------- 14.4/375.7 MB 13.7 MB/s eta 0:00:27\n",
      "   - -------------------------------------- 17.0/375.7 MB 13.6 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 19.4/375.7 MB 13.5 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 22.0/375.7 MB 13.4 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 24.6/375.7 MB 13.3 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 27.3/375.7 MB 13.3 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 29.9/375.7 MB 13.2 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 32.5/375.7 MB 13.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 35.1/375.7 MB 13.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 37.7/375.7 MB 13.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 40.4/375.7 MB 13.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 43.0/375.7 MB 13.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 45.9/375.7 MB 13.0 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 48.5/375.7 MB 13.0 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 51.1/375.7 MB 13.0 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 53.7/375.7 MB 13.0 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 56.4/375.7 MB 13.0 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 58.7/375.7 MB 12.9 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 61.1/375.7 MB 12.9 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 64.0/375.7 MB 12.9 MB/s eta 0:00:25\n",
      "   ------- -------------------------------- 66.6/375.7 MB 12.9 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 69.5/375.7 MB 12.9 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 72.4/375.7 MB 12.9 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 75.0/375.7 MB 12.9 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 77.6/375.7 MB 12.9 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 80.2/375.7 MB 12.9 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 82.8/375.7 MB 12.9 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 85.5/375.7 MB 12.9 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 88.1/375.7 MB 12.9 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 90.7/375.7 MB 12.9 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 93.6/375.7 MB 12.9 MB/s eta 0:00:22\n",
      "   ---------- ----------------------------- 96.2/375.7 MB 12.9 MB/s eta 0:00:22\n",
      "   ---------- ----------------------------- 99.1/375.7 MB 12.9 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 101.7/375.7 MB 12.9 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 104.3/375.7 MB 12.9 MB/s eta 0:00:22\n",
      "   ----------- --------------------------- 107.0/375.7 MB 12.9 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 109.6/375.7 MB 12.8 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 112.5/375.7 MB 12.8 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 115.1/375.7 MB 12.8 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 117.7/375.7 MB 12.8 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 120.3/375.7 MB 12.8 MB/s eta 0:00:20\n",
      "   ------------ -------------------------- 122.9/375.7 MB 12.9 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 125.6/375.7 MB 12.9 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 128.2/375.7 MB 12.8 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 130.8/375.7 MB 12.8 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 133.4/375.7 MB 12.8 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 136.1/375.7 MB 12.8 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 138.7/375.7 MB 12.8 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 141.3/375.7 MB 12.8 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 143.9/375.7 MB 12.8 MB/s eta 0:00:19\n",
      "   --------------- ----------------------- 146.5/375.7 MB 12.8 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 149.2/375.7 MB 12.8 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 151.8/375.7 MB 12.8 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 154.4/375.7 MB 12.8 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 157.0/375.7 MB 12.8 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 159.6/375.7 MB 12.8 MB/s eta 0:00:17\n",
      "   ---------------- ---------------------- 162.3/375.7 MB 12.8 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 164.9/375.7 MB 12.8 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 167.5/375.7 MB 12.8 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 170.1/375.7 MB 12.8 MB/s eta 0:00:17\n",
      "   ----------------- --------------------- 172.8/375.7 MB 12.8 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 175.4/375.7 MB 12.8 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 178.0/375.7 MB 12.8 MB/s eta 0:00:16\n",
      "   ------------------ -------------------- 180.9/375.7 MB 12.8 MB/s eta 0:00:16\n",
      "   ------------------- ------------------- 183.5/375.7 MB 12.8 MB/s eta 0:00:16\n",
      "   ------------------- ------------------- 186.1/375.7 MB 12.8 MB/s eta 0:00:15\n",
      "   ------------------- ------------------- 188.7/375.7 MB 12.8 MB/s eta 0:00:15\n",
      "   ------------------- ------------------- 191.4/375.7 MB 12.8 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 194.0/375.7 MB 12.8 MB/s eta 0:00:15\n",
      "   -------------------- ------------------ 196.6/375.7 MB 12.8 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 199.2/375.7 MB 12.8 MB/s eta 0:00:14\n",
      "   -------------------- ------------------ 201.3/375.7 MB 12.8 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 204.5/375.7 MB 12.8 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 207.1/375.7 MB 12.8 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 209.7/375.7 MB 12.8 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 212.3/375.7 MB 12.8 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 215.0/375.7 MB 12.8 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 217.3/375.7 MB 12.8 MB/s eta 0:00:13\n",
      "   ---------------------- ---------------- 219.9/375.7 MB 12.8 MB/s eta 0:00:13\n",
      "   ----------------------- --------------- 222.8/375.7 MB 12.8 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 225.4/375.7 MB 12.8 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 228.1/375.7 MB 12.8 MB/s eta 0:00:12\n",
      "   ----------------------- --------------- 230.7/375.7 MB 12.8 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 233.3/375.7 MB 12.8 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 235.9/375.7 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------------------ -------------- 238.6/375.7 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 241.2/375.7 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 243.8/375.7 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 246.4/375.7 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 249.0/375.7 MB 12.8 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 251.7/375.7 MB 12.8 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 254.3/375.7 MB 12.8 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 256.9/375.7 MB 12.8 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 258.5/375.7 MB 12.7 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 260.6/375.7 MB 12.7 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 263.7/375.7 MB 12.7 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 266.6/375.7 MB 12.7 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 269.2/375.7 MB 12.7 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 271.8/375.7 MB 12.7 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 274.5/375.7 MB 12.7 MB/s eta 0:00:08\n",
      "   ---------------------------- ---------- 277.3/375.7 MB 12.7 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 280.0/375.7 MB 12.7 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 282.6/375.7 MB 12.7 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 285.2/375.7 MB 12.7 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 287.8/375.7 MB 12.7 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 290.5/375.7 MB 12.7 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 293.1/375.7 MB 12.7 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 295.7/375.7 MB 12.7 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 298.3/375.7 MB 12.7 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 300.9/375.7 MB 12.7 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 303.8/375.7 MB 12.7 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 306.4/375.7 MB 12.7 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 309.1/375.7 MB 12.7 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 311.4/375.7 MB 12.7 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 314.0/375.7 MB 12.7 MB/s eta 0:00:05\n",
      "   -------------------------------- ------ 316.7/375.7 MB 12.7 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 319.6/375.7 MB 12.7 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 322.2/375.7 MB 12.7 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 324.8/375.7 MB 12.7 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 327.4/375.7 MB 12.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 330.0/375.7 MB 12.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 332.7/375.7 MB 12.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 335.3/375.7 MB 12.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 337.9/375.7 MB 12.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 340.5/375.7 MB 12.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 343.1/375.7 MB 12.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 345.8/375.7 MB 12.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 348.1/375.7 MB 12.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 351.0/375.7 MB 12.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 352.8/375.7 MB 12.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 356.3/375.7 MB 12.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 358.1/375.7 MB 12.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 361.5/375.7 MB 12.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 364.1/375.7 MB 12.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  366.7/375.7 MB 12.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  369.4/375.7 MB 12.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  372.0/375.7 MB 12.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  374.6/375.7 MB 12.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.7 MB 12.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.7 MB 12.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.7 MB 12.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 375.7/375.7 MB 12.3 MB/s eta 0:00:00\n",
      "Using cached absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.73.0-cp39-cp39-win_amd64.whl (4.3 MB)\n",
      "Using cached keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Installing collected packages: markdown-it-py, grpcio, google-pasta, gast, astunparse, absl-py, rich, markdown, tensorboard, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.0 astunparse-1.6.3 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.0 keras-3.10.0 markdown-3.8.2 markdown-it-py-3.0.0 rich-14.0.0 tensorboard-2.19.0 tensorflow-2.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk scikit-learn pandas tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "266jccQTQpcb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMLpD00NQu_y",
    "outputId": "2b715ea0-8d87-40bd-b5c0-3783c333b5f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     D:\\Users\\Bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     D:\\Users\\Bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     D:\\Users\\Bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     D:\\Users\\Bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preparation and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJ_7HPX-Regz"
   },
   "source": [
    "## a. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the text data, perform lowercasing and exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQWYZgQsTZm-"
   },
   "source": [
    "Read amazon review file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "YQ1yoq2bTdqO",
    "outputId": "e2b04647-2bdd-44b0-9c10-d57858b041e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews : 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Great CD</td>\n",
       "      <td>My lovely Pat has one of the GREAT voices of h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "      <td>Despite the fact that I have only played a sma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Batteries died within a year ...</td>\n",
       "      <td>I bought this charger in Jul 2003 and it worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>works fine, but Maha Energy is better</td>\n",
       "      <td>Check out Maha Energy's website. Their Powerex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for the non-audiophile</td>\n",
       "      <td>Reviewed quite a bit of the combo players and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                              title  \\\n",
       "0         2                                           Great CD   \n",
       "1         2  One of the best game music soundtracks - for a...   \n",
       "2         1                   Batteries died within a year ...   \n",
       "3         2              works fine, but Maha Energy is better   \n",
       "4         2                       Great for the non-audiophile   \n",
       "\n",
       "                                              review  \n",
       "0  My lovely Pat has one of the GREAT voices of h...  \n",
       "1  Despite the fact that I have only played a sma...  \n",
       "2  I bought this charger in Jul 2003 and it worke...  \n",
       "3  Check out Maha Energy's website. Their Powerex...  \n",
       "4  Reviewed quite a bit of the combo players and ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('amazon_review.csv')\n",
    "print(\"Number of reviews :\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzMYDgifT6fz"
   },
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4tLNTztT-Oi",
    "outputId": "933b99ff-8ce6-4bf5-af11-24759f01f4d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   polarity  50000 non-null  int64 \n",
      " 1   title     49996 non-null  object\n",
      " 2   review    50000 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bl7NnrHhUZTg"
   },
   "source": [
    "Dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3rcU5EtMUch4",
    "outputId": "ec42d514-f909-4a90-f1d1-68d32932fc8f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My lovely Pat has one of the GREAT voices of h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Despite the fact that I have only played a sma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I bought this charger in Jul 2003 and it worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Check out Maha Energy's website. Their Powerex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reviewed quite a bit of the combo players and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  My lovely Pat has one of the GREAT voices of h...\n",
       "1  Despite the fact that I have only played a sma...\n",
       "2  I bought this charger in Jul 2003 and it worke...\n",
       "3  Check out Maha Energy's website. Their Powerex...\n",
       "4  Reviewed quite a bit of the combo players and ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['polarity', 'title'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPKnaqTqUqEF"
   },
   "source": [
    "Lowercasing review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "SsHB7M8_Usry",
    "outputId": "570d9a04-8871-4f81-f3ca-a237c181513f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my lovely pat has one of the great voices of h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>despite the fact that i have only played a sma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i bought this charger in jul 2003 and it worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>check out maha energy's website. their powerex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reviewed quite a bit of the combo players and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  my lovely pat has one of the great voices of h...\n",
       "1  despite the fact that i have only played a sma...\n",
       "2  i bought this charger in jul 2003 and it worke...\n",
       "3  check out maha energy's website. their powerex...\n",
       "4  reviewed quite a bit of the combo players and ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show dataset statistics: number of sentences, vocabulary size, POS tag distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJKwnLk0VGi3"
   },
   "source": [
    "Flattening sentences in review comments for better analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IREko0oYWMf8"
   },
   "outputs": [],
   "source": [
    "sentences = [nltk.sent_tokenize(text) for text in df['review'].tolist()]\n",
    "flat_sentences = [sent for sublist in sentences for sent in sublist]\n",
    "num_sentences = len(flat_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sow79abUWn5V",
    "outputId": "64c280bf-d2bc-46f3-91d2-7ebc8a118e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences: 50000 <class 'list'> ['my lovely pat has one of the great voices of her generation.', 'i have listened to this cd for years and i still love it.', \"when i'm in a good mood it makes me feel better.\", 'a bad mood just evaporates like sugar in the rain.', 'this cd just oozes life.', 'vocals are jusat stuunning and lyrics just kill.', \"one of life's hidden gems.\", 'this is a desert isle cd in my book.', 'why she never made it big is just beyond me.', 'everytime i play this, no matter black, white, young, old, male, female everybody says one thing \"who was that singing ?\"']\n",
      "flat_sentences: 234928 <class 'list'> my lovely pat has one of the great voices of her generation.\n"
     ]
    }
   ],
   "source": [
    "print(\"sentences:\",len(sentences), type(sentences), sentences[0])\n",
    "print(\"flat_sentences:\",len(flat_sentences), type(flat_sentences), flat_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle special characters and normalize text appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bwCgVFz3VGw3",
    "outputId": "2bfc2b8c-cf22-4499-f090-1787d6c70cc8"
   },
   "outputs": [],
   "source": [
    "cleaned_sentences = [re.sub(r'[^A-Za-z0-9\\s]', '', sent).strip() for sent in flat_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BvXLVGsFWjqR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_sentences: 234928 <class 'list'> my lovely pat has one of the great voices of her generation\n"
     ]
    }
   ],
   "source": [
    "print(\"cleaned_sentences:\",len(cleaned_sentences), type(cleaned_sentences), cleaned_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization for words to get vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: 115935 <class 'set'>\n"
     ]
    }
   ],
   "source": [
    "words = [nltk.word_tokenize(sentence) for sentence in cleaned_sentences]\n",
    "flat_words = [word for sublist in words for word in sublist]\n",
    "vocabulary = set(flat_words)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(\"vocabulary:\",len(vocabulary), type(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flat_pos_tags: 3768512 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "pos_tags = [nltk.pos_tag(sentence_words) for sentence_words in words]\n",
    "flat_pos_tags = [tag for sublist in pos_tags for word, tag in sublist]\n",
    "pos_tag_counts = Counter(flat_pos_tags)\n",
    "print(\"flat_pos_tags:\",len(flat_pos_tags), type(flat_pos_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Summary of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "Number of reviews : 50000\n",
      "Number of sentences: 234928\n",
      "Vocabulary size: 115935\n",
      "all POS tags: 3768512\n",
      "POS tag distribution: [('NN', 750012), ('DT', 432134), ('IN', 404164), ('JJ', 343414), ('RB', 245255), ('NNS', 193329), ('PRP', 166052), ('VB', 162321), ('CC', 145798), ('VBZ', 135285)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Statistics:\")\n",
    "print(\"Number of reviews :\", len(df))\n",
    "print(f\"Number of sentences: {num_sentences}\")\n",
    "print(f\"Vocabulary size: {vocabulary_size}\")\n",
    "print(f\"all POS tags: {len(flat_pos_tags)}\")\n",
    "print(f\"POS tag distribution: {pos_tag_counts.most_common(10)}\") # Show top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting POS Tag Counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unpack the tags and their counts\n",
    "tags, counts = zip(*pos_tag_counts.items())\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(tags, counts, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('POS Tags')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Part-of-Speech Tag Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Tokenization, Encoding and Sequence Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement tokenization for words and sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word and tag lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_with_tags = [(word, tag) for sent in pos_tags for word, tag in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate word-to-index and tag-to-index mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate word to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: i + 1 for i, word in enumerate(sorted(list(vocabulary)))} # Add 1 for padding\n",
    "word_to_index['<PAD>'] = 0 # Padding token\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Tag to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = sorted(list(set(flat_pos_tags)))\n",
    "tag_to_index = {tag: i + 1 for i, tag in enumerate(all_tags)} # Add 1 for padding\n",
    "tag_to_index['<PAD>'] = 0 # Padding tag\n",
    "index_to_tag = {i: tag for tag, i in tag_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary mappings and POS tag encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = []\n",
    "for sent in pos_tags:\n",
    "    word_indices = [word_to_index.get(word, word_to_index['<PAD>']) for word, tag in sent]\n",
    "    tag_indices = [tag_to_index.get(tag, tag_to_index['<PAD>']) for word, tag in sent]\n",
    "    encoded_sequences.append((word_indices, tag_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training (70%), validation (15%), and test (15%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences, temp_sequences = train_test_split(encoded_sequences, test_size=0.30, random_state=42)\n",
    "val_sequences, test_sequences = train_test_split(temp_sequences, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mapping Sizes:\n",
      "Word to index mapping size: 115936\n",
      "Tag to index mapping size: 39\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMapping Sizes:\")\n",
    "print(f\"Word to index mapping size: {len(word_to_index)}\")\n",
    "print(f\"Tag to index mapping size: {len(tag_to_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Split:\n",
      "Number of training sequences: 164449\n",
      "Number of validation sequences: 35239\n",
      "Number of test sequences: 35240\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData Split:\")\n",
    "print(f\"Number of training sequences: {len(train_sequences)}\")\n",
    "print(f\"Number of validation sequences: {len(val_sequences)}\")\n",
    "print(f\"Number of test sequences: {len(test_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Encoded Sentence:\n",
      "Word Indices: [68285, 60946, 75278, 47051, 72204, 71580, 101956, 45017, 110390, 71580] ...\n",
      "Tag Indices: [21, 9, 14, 34, 4, 8, 5, 9, 17, 8] ...\n",
      "Original Words: ['my', 'lovely', 'pat', 'has', 'one', 'of', 'the', 'great', 'voices', 'of'] ...\n",
      "Original Tags: ['PRP$', 'JJ', 'NN', 'VBZ', 'CD', 'IN', 'DT', 'JJ', 'NNS', 'IN'] ...\n"
     ]
    }
   ],
   "source": [
    "if encoded_sequences:\n",
    "    sample_word_indices, sample_tag_indices = encoded_sequences[0]\n",
    "    print(\"\\nExample Encoded Sentence:\")\n",
    "    print(\"Word Indices:\", sample_word_indices[:10], \"...\")\n",
    "    print(\"Tag Indices:\", sample_tag_indices[:10], \"...\")\n",
    "    print(\"Original Words:\", [index_to_word.get(idx, '<UNK>') for idx in sample_word_indices[:10]], \"...\")\n",
    "    print(\"Original Tags:\", [index_to_tag.get(idx, '<UNK>') for idx in sample_tag_indices[:10]], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Feature Extraction for MEMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts features for each word in a sentence for MEMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sentence, word_index, tag_index, prev_tags=None):\n",
    "    \"\"\"\n",
    "    Extracts features for each word in a sentence for MEMM.\n",
    "\n",
    "    Args:\n",
    "        sentence (list): A list of (word, tag) tuples for a sentence.\n",
    "        word_index (dict): Mapping from word to index.\n",
    "        tag_index (dict): Mapping from tag to index.\n",
    "        prev_tags (list, optional): A list of previous tags (indices)\n",
    "                                     for the sequence. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of feature dictionaries, one for each word in the sentence.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    sentence_length = len(sentence)\n",
    "\n",
    "    # Ensure prev_tags is initialized as a list of padding indices if not provided\n",
    "    if prev_tags is None:\n",
    "        prev_tags = [tag_index.get('<PAD>', 0)] * sentence_length # Placeholder, will be updated dynamically\n",
    "\n",
    "    for i in range(sentence_length):\n",
    "        word, current_tag = sentence[i]\n",
    "        word_idx = word_to_index.get(word, word_to_index.get('<PAD>', 0))\n",
    "        tag_idx = tag_to_index.get(current_tag, tag_to_index.get('<PAD>', 0))\n",
    "\n",
    "        # Handle previous tags dynamically or using the provided sequence\n",
    "        prev_tag_idx = prev_tags[i-1] if i > 0 else tag_index.get('<PAD>', 0)\n",
    "        prev_prev_tag_idx = prev_tags[i-2] if i > 1 else tag_index.get('<PAD>', 0)\n",
    "\n",
    "        feature_dict = {\n",
    "            # Current word and its properties\n",
    "            'word': word,\n",
    "            'word_idx': word_idx,\n",
    "            'word.lower()': word.lower(),\n",
    "            'word.isupper()': word.isupper(),\n",
    "            'word.istitle()': word.istitle(),\n",
    "            'word.isdigit()': word.isdigit(),\n",
    "            'word_length': len(word),\n",
    "\n",
    "            # Context features\n",
    "            'prev_word': sentence[i-1][0] if i > 0 else '<START>',\n",
    "            'prev_word_idx': word_to_index.get(sentence[i-1][0], word_to_index.get('<PAD>', 0)) if i > 0 else word_to_index.get('<PAD>', 0),\n",
    "            'next_word': sentence[i+1][0] if i < sentence_length - 1 else '<END>',\n",
    "            'next_word_idx': word_to_index.get(sentence[i+1][0], word_to_index.get('<PAD>', 0)) if i < sentence_length - 1 else word_to_index.get('<PAD>', 0),\n",
    "            'prev_tag_idx': prev_tag_idx, # This should ideally be the predicted tag\n",
    "            'prev_prev_tag_idx': prev_prev_tag_idx, # This should ideally be the predicted tag\n",
    "\n",
    "            # Morphological features\n",
    "            'prefix-1': word[0] if len(word) > 0 else '',\n",
    "            'prefix-2': word[:2] if len(word) > 1 else '',\n",
    "            'prefix-3': word[:3] if len(word) > 2 else '',\n",
    "            'suffix-1': word[-1] if len(word) > 0 else '',\n",
    "            'suffix-2': word[-2:] if len(word) > 1 else '',\n",
    "            'suffix-3': word[-3:] if len(word) > 2 else '',\n",
    "            'word_shape': re.sub(r'[A-Z]', 'X', re.sub(r'[a-z]', 'x', re.sub(r'[0-9]', 'd', word))),\n",
    "\n",
    "            # Position features\n",
    "            'is_first': i == 0,\n",
    "            'is_last': i == sentence_length - 1,\n",
    "\n",
    "            # The target variable (current tag) is NOT a feature for MEMM,\n",
    "            # but we might store it for convenience during training.\n",
    "            # 'current_tag_idx': tag_idx\n",
    "        }\n",
    "        features.append(feature_dict)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage on a sample sentence from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_sentence_with_tags 4 <class 'list'> [('bad', 'JJ'), ('product', 'NN'), ('bad', 'JJ'), ('company', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Example usage on a sample sentence from the training data\n",
    "# We need the original (word, tag) tuples to extract features\n",
    "# Let's reconstruct a sample sentence from the first encoded training sequence\n",
    "sample_encoded_sequence = train_sequences[0]\n",
    "sample_word_indices, sample_tag_indices = sample_encoded_sequence\n",
    "sample_sentence_with_tags = [(index_to_word.get(w_idx, '<UNK>'), index_to_tag.get(t_idx, '<UNK>'))\n",
    "                             for w_idx, t_idx in zip(sample_word_indices, sample_tag_indices)]\n",
    "\n",
    "print(\"sample_sentence_with_tags\",len(sample_sentence_with_tags),type(sample_sentence_with_tags), sample_sentence_with_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For feature extraction in the training phase, we use the true previous tags.\n",
    "# For prediction (inference), we will use the previously predicted tags.\n",
    "# Let's get the actual previous tag indices for the sample sentence\n",
    "sample_true_prev_tag_indices = [tag_to_index.get(tag, tag_to_index.get('<PAD>', 0)) for _, tag in sample_sentence_with_tags]\n",
    "sample_features = extract_features(sample_sentence_with_tags, word_to_index, tag_to_index, prev_tags=sample_true_prev_tag_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features for the first few words of a sample sentence:\n",
      "Word: bad\n",
      "{'word': 'bad', 'word.isupper()': False, 'prev_word': '<START>', 'next_word': 'product', 'prev_tag_idx': 0, 'is_first': True, 'suffix-3': 'bad'}\n",
      "--------------------\n",
      "Word: product\n",
      "{'word': 'product', 'word.isupper()': False, 'prev_word': 'bad', 'next_word': 'bad', 'prev_tag_idx': 9, 'is_first': False, 'suffix-3': 'uct'}\n",
      "--------------------\n",
      "Word: bad\n",
      "{'word': 'bad', 'word.isupper()': False, 'prev_word': 'product', 'next_word': 'company', 'prev_tag_idx': 14, 'is_first': False, 'suffix-3': 'bad'}\n",
      "--------------------\n",
      "Word: company\n",
      "{'word': 'company', 'word.isupper()': False, 'prev_word': 'bad', 'next_word': '<END>', 'prev_tag_idx': 9, 'is_first': False, 'suffix-3': 'any'}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFeatures for the first few words of a sample sentence:\")\n",
    "for j, feature in enumerate(sample_features[:5]):\n",
    "    print(f\"Word: {sample_sentence_with_tags[j][0]}\")\n",
    "    # Print a subset of features for clarity\n",
    "    print({k: feature[k] for k in ['word', 'word.isupper()', 'prev_word', 'next_word', 'prev_tag_idx', 'is_first', 'suffix-3']})\n",
    "    print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Maximum Entropy Markov Model (MEMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates feature vectors (X) and labels (y) for MEMM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_memm_data(sequences):\n",
    "    \"\"\"\n",
    "    Creates feature vectors (X) and labels (y) for MEMM training.\n",
    "    Features are extracted using true previous tags during training.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for word_indices, tag_indices in sequences:\n",
    "        sentence_words_tags = [(index_to_word.get(w_idx, '<UNK>'), index_to_tag.get(t_idx, '<UNK>'))\n",
    "                               for w_idx, t_idx in zip(word_indices, tag_indices)]\n",
    "        # Extract features using true previous tags for training\n",
    "        sentence_features = extract_features(sentence_words_tags, word_to_index, tag_to_index, prev_tags=tag_indices)\n",
    "        X.extend(sentence_features)\n",
    "        y.extend([tag_to_index.get(tag, tag_to_index.get('<PAD>', 0)) for _, tag in sentence_words_tags])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for MEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_memm, y_train_memm = create_memm_data(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_memm, y_val_memm = create_memm_data(val_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_memm, y_test_memm = create_memm_data(test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating MEMM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the MEMM pipeline\n",
    "# 1. DictVectorizer to convert dictionaries of features into a numerical feature matrix\n",
    "# 2. LogisticRegression for the classification of each word's tag\n",
    "memm_pipeline = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=True)), # sparse=True is efficient for sparse features\n",
    "    ('classifier', LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=200, C=1.0, penalty='l2', n_jobs=-1, verbose=1 )) # Add regularization\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the MEMM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "memm_pipeline.fit(X_train_memm, y_train_memm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token-level accuracy on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_token = memm_pipeline.predict(X_val_memm)\n",
    "token_accuracy_memm = accuracy_score(y_val_memm, y_val_pred_token)\n",
    "print(f\"MEMM Token-level accuracy on validation set: {token_accuracy_memm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_memm(sentence_words, memm_pipeline, word_to_index, tag_to_index, index_to_tag):\n",
    "    \"\"\"\n",
    "    Performs Viterbi decoding for MEMM.\n",
    "\n",
    "    Args:\n",
    "        sentence_words (list): A list of words in the sentence.\n",
    "        memm_pipeline: Trained MEMM pipeline (DictVectorizer + LogisticRegression).\n",
    "        word_to_index (dict): Mapping from word to index.\n",
    "        tag_to_index (dict): Mapping from tag to index.\n",
    "        index_to_tag (dict): Mapping from index to tag.\n",
    "\n",
    "    Returns:\n",
    "        list: The predicted sequence of tags for the sentence.\n",
    "    \"\"\"\n",
    "    num_tags = len(tag_to_index)\n",
    "    sentence_length = len(sentence_words)\n",
    "\n",
    "    # Initialize DP table (delta[i][k] = max probability of a tag sequence ending with tag k at word i)\n",
    "    delta = np.zeros((sentence_length, num_tags))\n",
    "    # Initialize backpointer table (psi[i][k] = index of the previous tag that maximizes probability)\n",
    "    psi = np.zeros((sentence_length, num_tags), dtype=int)\n",
    "\n",
    "    # Get the index for the padding tag, which we'll treat as the start tag\n",
    "    pad_tag_idx = tag_to_index.get('<PAD>', 0)\n",
    "    # Get all possible tag indices (excluding padding for emissions/transitions)\n",
    "    all_tag_indices = [idx for tag, idx in tag_to_index.items() if tag != '<PAD>']\n",
    "    start_tag_idx = pad_tag_idx # Use padding tag index as a placeholder for start\n",
    "\n",
    "    # For the first word (i=0)\n",
    "    current_word = sentence_words[0]\n",
    "    # Features for the first word (using <START> for previous word/tag)\n",
    "    features_0 = extract_features([(current_word, '<PAD>')], word_to_index, tag_to_index, prev_tags=[start_tag_idx])[0]\n",
    "    # Convert features to numerical vector\n",
    "    X_0 = memm_pipeline.named_steps['vectorizer'].transform([features_0])\n",
    "\n",
    "    # Get log probabilities for each tag for the first word\n",
    "    # P(tag_0 | features_0)\n",
    "    log_probs_0 = memm_pipeline.named_steps['classifier'].predict_log_proba(X_0)[0]\n",
    "\n",
    "    # delta[0][k] = P(tag_0 = k | features_0)\n",
    "    for k in all_tag_indices:\n",
    "         # Ensure the tag index exists in the model's classes_\n",
    "        if k in memm_pipeline.named_steps['classifier'].classes_:\n",
    "            # Find the column index for the current tag index k in the log_probs_0 array\n",
    "            col_index = np.where(memm_pipeline.named_steps['classifier'].classes_ == k)[0]\n",
    "            if len(col_index) > 0:\n",
    "                 delta[0][k] = log_probs_0[col_index[0]]\n",
    "            else:\n",
    "                 delta[0][k] = -np.inf # Tag not in classifier's vocabulary\n",
    "        else:\n",
    "             delta[0][k] = -np.inf # Tag not in classifier's vocabulary\n",
    "\n",
    "\n",
    "    # For words i = 1 to sentence_length - 1\n",
    "    for i in range(1, sentence_length):\n",
    "        current_word = sentence_words[i]\n",
    "\n",
    "        for k in all_tag_indices: # Iterate over possible current tags k\n",
    "             # Ensure the tag index exists in the model's classes_\n",
    "            if k not in memm_pipeline.named_steps['classifier'].classes_:\n",
    "                 delta[i][k] = -np.inf\n",
    "                 continue # Skip if the tag is not in the classifier's output classes\n",
    "\n",
    "            max_prob = -np.inf\n",
    "            best_prev_tag_idx = -1\n",
    "\n",
    "            for j in all_tag_indices: # Iterate over possible previous tags j\n",
    "                 # Ensure the previous tag index exists in the model's classes_\n",
    "                if delta[i-1][j] == -np.inf:\n",
    "                    continue # Skip if the probability of ending with tag j at i-1 is zero\n",
    "\n",
    "                # Features for the current word, using the assumed previous tag j\n",
    "                # Note: Here's where MEMM differs. Features include the *assumed* previous tag.\n",
    "                features_i = extract_features([(current_word, '<PAD>')], word_to_index, tag_to_index, prev_tags=[pad_tag_idx] * (i) + [j] + [pad_tag_idx] * (sentence_length - i - 1))[0]\n",
    "                # Convert features to numerical vector\n",
    "                X_i = memm_pipeline.named_steps['vectorizer'].transform([features_i])\n",
    "\n",
    "                # Get log probabilities for each tag for the current word\n",
    "                # P(tag_i | tag_{i-1}=j, features_i)\n",
    "                log_probs_i = memm_pipeline.named_steps['classifier'].predict_log_proba(X_i)[0]\n",
    "\n",
    "                # Find the log probability of the current tag k given features_i\n",
    "                # This is log P(tag_i=k | tag_{i-1}=j, features_i)\n",
    "                col_index_k = np.where(memm_pipeline.named_steps['classifier'].classes_ == k)[0]\n",
    "                if len(col_index_k) > 0:\n",
    "                    log_prob_current_tag = log_probs_i[col_index_k[0]]\n",
    "                else:\n",
    "                    log_prob_current_tag = -np.inf # Should not happen if k is in all_tag_indices and model classes\n",
    "\n",
    "                # Probability of the path ending with tag j at i-1 and tag k at i\n",
    "                current_path_prob = delta[i-1][j] + log_prob_current_tag\n",
    "\n",
    "                if current_path_prob > max_prob:\n",
    "                    max_prob = current_path_prob\n",
    "                    best_prev_tag_idx = j\n",
    "\n",
    "            delta[i][k] = max_prob\n",
    "            psi[i][k] = best_prev_tag_idx\n",
    "\n",
    "    # Backtrack to find the best tag sequence\n",
    "    best_sequence = [0] * sentence_length\n",
    "\n",
    "    # Find the last tag (tag at sentence_length - 1)\n",
    "    best_last_tag_idx = np.argmax(delta[sentence_length - 1, all_tag_indices])\n",
    "    best_sequence[sentence_length - 1] = all_tag_indices[best_last_tag_idx]\n",
    "\n",
    "    # Backtrack from sentence_length - 2 down to 0\n",
    "    for i in range(sentence_length - 2, -1, -1):\n",
    "        best_sequence[i] = psi[i+1][best_sequence[i+1]]\n",
    "\n",
    "    # Convert tag indices back to tag strings\n",
    "    predicted_tags = [index_to_tag.get(idx, '<UNK>') for idx in best_sequence]\n",
    "\n",
    "    return predicted_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of Viterbi decoding on a sample test sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This DictVectorizer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m sample_test_words \u001b[38;5;241m=\u001b[39m [index_to_word\u001b[38;5;241m.\u001b[39mget(w_idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m w_idx \u001b[38;5;129;01min\u001b[39;00m sample_test_word_indices]\n\u001b[0;32m      4\u001b[0m sample_true_tags \u001b[38;5;241m=\u001b[39m [index_to_tag\u001b[38;5;241m.\u001b[39mget(t_idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<UNK>\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t_idx \u001b[38;5;129;01min\u001b[39;00m sample_test_tag_indices]\n\u001b[1;32m----> 6\u001b[0m predicted_tags_memm \u001b[38;5;241m=\u001b[39m \u001b[43mviterbi_memm\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_test_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemm_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_to_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag_to_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_to_tag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMEMM Viterbi Decoding Example:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_test_words)\n",
      "Cell \u001b[1;32mIn[38], line 34\u001b[0m, in \u001b[0;36mviterbi_memm\u001b[1;34m(sentence_words, memm_pipeline, word_to_index, tag_to_index, index_to_tag)\u001b[0m\n\u001b[0;32m     32\u001b[0m features_0 \u001b[38;5;241m=\u001b[39m extract_features([(current_word, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m)], word_to_index, tag_to_index, prev_tags\u001b[38;5;241m=\u001b[39m[start_tag_idx])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Convert features to numerical vector\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m X_0 \u001b[38;5;241m=\u001b[39m \u001b[43mmemm_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvectorizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures_0\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Get log probabilities for each tag for the first word\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# P(tag_0 | features_0)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m log_probs_0 \u001b[38;5;241m=\u001b[39m memm_pipeline\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict_log_proba(X_0)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\feature_extraction\\_dict_vectorizer.py:382\u001b[0m, in \u001b[0;36mDictVectorizer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    366\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform feature->value dicts to array or sparse matrix.\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \n\u001b[0;32m    368\u001b[0m \u001b[38;5;124;03m    Named features not encountered during fit or fit_transform will be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        Feature vectors; always 2-d.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature_names_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvocabulary_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(X, fitting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\users\\bhavi\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1757\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This DictVectorizer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "sample_test_encoded = test_sequences[0]\n",
    "sample_test_word_indices, sample_test_tag_indices = sample_test_encoded\n",
    "sample_test_words = [index_to_word.get(w_idx, '<UNK>') for w_idx in sample_test_word_indices]\n",
    "sample_true_tags = [index_to_tag.get(t_idx, '<UNK>') for t_idx in sample_test_tag_indices]\n",
    "\n",
    "predicted_tags_memm = viterbi_memm(sample_test_words, memm_pipeline, word_to_index, tag_to_index, index_to_tag)\n",
    "\n",
    "print(\"\\nMEMM Viterbi Decoding Example:\")\n",
    "print(\"Sentence:\", sample_test_words)\n",
    "print(\"True Tags:\", sample_true_tags)\n",
    "print(\"Predicted Tags (MEMM):\", predicted_tags_memm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate MEMM Viterbi decoding on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_memm_viterbi(test_sequences, memm_pipeline, word_to_index, tag_to_index, index_to_tag):\n",
    "    correct_tags = 0\n",
    "    total_tags = 0\n",
    "    for word_indices, tag_indices in test_sequences:\n",
    "        sentence_words = [index_to_word.get(w_idx, '<UNK>') for w_idx in word_indices]\n",
    "        true_tags = [index_to_tag.get(t_idx, '<UNK>') for t_idx in tag_indices]\n",
    "\n",
    "        # Skip sentences with only padding or empty\n",
    "        if not sentence_words or all(word == '<UNK>' or word == '<PAD>' for word in sentence_words):\n",
    "            continue\n",
    "\n",
    "        predicted_tags = viterbi_memm(sentence_words, memm_pipeline, word_to_index, tag_to_index, index_to_tag)\n",
    "\n",
    "        # Compare predicted tags with true tags (ignoring padding)\n",
    "        for true_tag, predicted_tag in zip(true_tags, predicted_tags):\n",
    "            if true_tag != '<PAD>': # Do not count padding in accuracy calculation\n",
    "                 if true_tag == predicted_tag:\n",
    "                    correct_tags += 1\n",
    "                 total_tags += 1\n",
    "\n",
    "    accuracy = correct_tags / total_tags if total_tags > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memm_viterbi_accuracy = evaluate_memm_viterbi(test_sequences, memm_pipeline, word_to_index, tag_to_index, index_to_tag)\n",
    "print(f\"MEMM Viterbi accuracy on test set: {memm_viterbi_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Traditional HMM Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating HMM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct word-tag sequences from encoded sequences for HMM\n",
    "train_word_tag_sequences = [\n",
    "    [(index_to_word.get(w_idx, '<UNK>'), index_to_tag.get(t_idx, '<UNK>'))\n",
    "     for w_idx, t_idx in zip(word_indices, tag_indices) if index_to_tag.get(t_idx, '<UNK>') != '<PAD>'] # Exclude padding\n",
    "    for word_indices, tag_indices in train_sequences\n",
    "    if any(index_to_tag.get(t_idx, '<UNK>') != '<PAD>' for t_idx in tag_indices) # Skip empty sentences after removing padding\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all tags and words from the training data (excluding padding)\n",
    "train_tags = [tag for sent in train_word_tag_sequences for word, tag in sent]\n",
    "train_words = [word for sent in train_word_tag_sequences for word, tag in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count frequencies\n",
    "tag_counts = Counter(train_tags)\n",
    "word_tag_counts = Counter([(word, tag) for sent in train_word_tag_sequences for word, tag in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate transition probabilities P(tag_i | tag_{i-1})\n",
    "transition_counts = Counter()\n",
    "for sent in train_word_tag_sequences:\n",
    "    # Add a virtual start tag\n",
    "    sentence_tags = ['<START>'] + [tag for word, tag in sent]\n",
    "    for i in range(1, len(sentence_tags)):\n",
    "        transition_counts[(sentence_tags[i-1], sentence_tags[i])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a virtual start tag count for calculating probabilities from START\n",
    "tag_counts['<START>'] = len(train_word_tag_sequences) # Number of sentences\n",
    "\n",
    "transition_probabilities = {}\n",
    "for (tag1, tag2), count in transition_counts.items():\n",
    "    transition_probabilities[(tag1, tag2)] = count / tag_counts[tag1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate emission probabilities P(word | tag)\n",
    "emission_probabilities = {}\n",
    "for (word, tag), count in word_tag_counts.items():\n",
    "    emission_probabilities[(word, tag)] = count / tag_counts[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement smoothing for unseen word-tag combinations (Add-k smoothing)\n",
    "k_smoothing = 0.1 # Small smoothing value\n",
    "all_words_set = set(train_words)\n",
    "all_tags_set = set(train_tags) # Excludes <START>\n",
    "vocabulary_size_hmm = len(all_words_set)\n",
    "num_tags_hmm = len(all_tags_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothed emission probabilities\n",
    "smoothed_emission_probabilities = {}\n",
    "# Iterate over all possible (word, tag) pairs to apply smoothing\n",
    "for word in all_words_set:\n",
    "    for tag in all_tags_set:\n",
    "        count = word_tag_counts.get((word, tag), 0)\n",
    "        tag_count = tag_counts.get(tag, 0) # Use 0 if tag somehow isn't in counts (shouldn't happen with how we collected train_tags)\n",
    "        # Apply add-k smoothing: (count + k) / (total_count_of_tag + k * vocabulary_size)\n",
    "        smoothed_emission_probabilities[(word, tag)] = (count + k_smoothing) / (tag_count + k_smoothing * vocabulary_size_hmm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For words not seen in training, use a small probability or handle specifically during Viterbi\n",
    "# We'll handle unseen words during Viterbi by giving them a small default emission probability.\n",
    "default_unseen_emission_prob = k_smoothing / (num_tags_hmm + k_smoothing * vocabulary_size_hmm) # A very small value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Viterbi algorithm for HMM\n",
    "def viterbi_hmm(sentence_words, transition_probabilities, emission_probabilities, tag_counts, all_tags_set, default_unseen_emission_prob):\n",
    "    \"\"\"\n",
    "    Performs Viterbi decoding for HMM.\n",
    "\n",
    "    Args:\n",
    "        sentence_words (list): A list of words in the sentence.\n",
    "        transition_probabilities (dict): P(tag_i | tag_{i-1}).\n",
    "        emission_probabilities (dict): P(word | tag).\n",
    "        tag_counts (Counter): Counts of each tag.\n",
    "        all_tags_set (set): Set of all unique tags.\n",
    "        default_unseen_emission_prob (float): Probability for unseen word-tag pairs.\n",
    "\n",
    "    Returns:\n",
    "        list: The predicted sequence of tags for the sentence.\n",
    "    \"\"\"\n",
    "    num_tags = len(all_tags_set)\n",
    "    tags_list = sorted(list(all_tags_set)) # Ordered list of tags\n",
    "    tag_to_idx_hmm = {tag: i for i, tag in enumerate(tags_list)}\n",
    "    idx_to_tag_hmm = {i: tag for tag, i in tag_to_idx_hmm.items()}\n",
    "\n",
    "    sentence_length = len(sentence_words)\n",
    "\n",
    "    if sentence_length == 0:\n",
    "        return []\n",
    "\n",
    "    # Initialize DP table (delta[i][k] = max probability of a tag sequence ending with tag k at word i)\n",
    "    delta = np.zeros((sentence_length, num_tags))\n",
    "    # Initialize backpointer table (psi[i][k] = index of the previous tag that maximizes probability)\n",
    "    psi = np.zeros((sentence_length, num_tags), dtype=int)\n",
    "\n",
    "    # For the first word (i=0)\n",
    "    word_0 = sentence_words[0]\n",
    "    for k_idx, tag_k in enumerate(tags_list):\n",
    "        # P(tag_0 = k | <START>) * P(word_0 | tag_0 = k)\n",
    "        prob_start_to_k = transition_probabilities.get(('<START>', tag_k), 0) # Use 0 if no transition from START\n",
    "        prob_emit_word0_from_k = emission_probabilities.get((word_0, tag_k), default_unseen_emission_prob)\n",
    "\n",
    "        delta[0][k_idx] = prob_start_to_k * prob_emit_word0_from_k\n",
    "\n",
    "    # For words i = 1 to sentence_length - 1\n",
    "    for i in range(1, sentence_length):\n",
    "        current_word = sentence_words[i]\n",
    "        for k_idx, tag_k in enumerate(tags_list): # Iterate over possible current tags tag_k\n",
    "             # Probability of emitting the current word from the current tag tag_k\n",
    "            prob_emit_wordi_from_k = emission_probabilities.get((current_word, tag_k), default_unseen_emission_prob)\n",
    "\n",
    "            max_prob = -1.0\n",
    "            best_prev_tag_idx = -1\n",
    "\n",
    "            for j_idx, tag_j in enumerate(tags_list): # Iterate over possible previous tags tag_j\n",
    "                # Probability of transitioning from previous tag tag_j to current tag tag_k\n",
    "                prob_transition_j_to_k = transition_probabilities.get((tag_j, tag_k), 0) # Use 0 for unseen transitions\n",
    "\n",
    "                # Probability of the path ending with tag tag_j at i-1 and tag tag_k at i\n",
    "                current_path_prob = delta[i-1][j_idx] * prob_transition_j_to_k * prob_emit_wordi_from_k\n",
    "\n",
    "                if current_path_prob > max_prob:\n",
    "                    max_prob = current_path_prob\n",
    "                    best_prev_tag_idx = j_idx\n",
    "\n",
    "            delta[i][k_idx] = max_prob\n",
    "            psi[i][k_idx] = best_prev_tag_idx\n",
    "\n",
    "    # Backtrack to find the best tag sequence\n",
    "    best_sequence_indices = [0] * sentence_length\n",
    "\n",
    "    # Find the index of the last tag (tag at sentence_length - 1) with the highest probability\n",
    "    best_last_tag_idx_in_list = np.argmax(delta[sentence_length - 1, :])\n",
    "    best_sequence_indices[sentence_length - 1] = best_last_tag_idx_in_list\n",
    "\n",
    "    # Backtrack from sentence_length - 2 down to 0\n",
    "    for i in range(sentence_length - 2, -1, -1):\n",
    "        best_sequence_indices[i] = psi[i+1][best_sequence_indices[i+1]]\n",
    "\n",
    "    # Convert tag indices back to tag strings\n",
    "    predicted_tags = [idx_to_tag_hmm.get(idx, '<UNK>') for idx in best_sequence_indices]\n",
    "\n",
    "    return predicted_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate HMM Viterbi decoding on the test set\n",
    "def evaluate_hmm_viterbi(test_sequences, transition_probabilities, emission_probabilities, tag_counts, all_tags_set, default_unseen_emission_prob, index_to_word, index_to_tag):\n",
    "    correct_tags = 0\n",
    "    total_tags = 0\n",
    "    for word_indices, tag_indices in test_sequences:\n",
    "        sentence_words = [index_to_word.get(w_idx, '<UNK>') for w_idx in word_indices]\n",
    "        true_tags = [index_to_tag.get(t_idx, '<UNK>') for t_idx in tag_indices]\n",
    "\n",
    "         # Skip sentences with only padding or empty\n",
    "        if not sentence_words or all(word == '<UNK>' or word == '<PAD>' for word in sentence_words):\n",
    "            continue\n",
    "\n",
    "        # Remove padding tags from true tags for evaluation comparison\n",
    "        true_tags_filtered = [tag for tag in true_tags if tag != '<PAD>']\n",
    "        sentence_words_filtered = [word for word, tag in zip(sentence_words, true_tags) if tag != '<PAD>']\n",
    "\n",
    "\n",
    "        if not sentence_words_filtered: # Skip if sentence is empty after filtering\n",
    "             continue\n",
    "\n",
    "        predicted_tags = viterbi_hmm(sentence_words_filtered, transition_probabilities, emission_probabilities, tag_counts, all_tags_set, default_unseen_emission_prob)\n",
    "\n",
    "        # Ensure predicted_tags and true_tags_filtered have the same length\n",
    "        # This should be true if viterbi_hmm returns a tag for every word\n",
    "        if len(predicted_tags) != len(true_tags_filtered):\n",
    "            print(f\"Warning: Length mismatch between predicted ({len(predicted_tags)}) and true ({len(true_tags_filtered)}) tags. Skipping sentence.\")\n",
    "            continue\n",
    "\n",
    "        # Compare predicted tags with true tags\n",
    "        for true_tag, predicted_tag in zip(true_tags_filtered, predicted_tags):\n",
    "            if true_tag == predicted_tag:\n",
    "                correct_tags += 1\n",
    "            total_tags += 1\n",
    "\n",
    "    accuracy = correct_tags / total_tags if total_tags > 0 else 0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate HMM Viterbi decoding on the test set\n",
    "print(\"\\nEvaluating HMM Viterbi accuracy on test set...\")\n",
    "hmm_viterbi_accuracy = evaluate_hmm_viterbi(test_sequences, transition_probabilities, smoothed_emission_probabilities, tag_counts, all_tags_set, default_unseen_emission_prob, index_to_word, index_to_tag)\n",
    "print(f\"HMM Viterbi accuracy on test set: {hmm_viterbi_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Neural Network POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the neural network\n",
    "embedding_dim = 50\n",
    "lstm_units = 64\n",
    "dense_units = 128\n",
    "vocab_size_nn = len(word_to_index)\n",
    "num_tags_nn = len(tag_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum sentence length for padding\n",
    "max_len = max(len(word_indices) for word_indices, _ in encoded_sequences)\n",
    "print(f\"Maximum sentence length for padding: {max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "X_train_nn = pad_sequences([seq[0] for seq in train_sequences], maxlen=max_len, padding='post')\n",
    "y_train_nn = pad_sequences([seq[1] for seq in train_sequences], maxlen=max_len, padding='post')\n",
    "\n",
    "X_val_nn = pad_sequences([seq[0] for seq in val_sequences], maxlen=max_len, padding='post')\n",
    "y_val_nn = pad_sequences([seq[1] for seq in val_sequences], maxlen=max_len, padding='post')\n",
    "\n",
    "X_test_nn = pad_sequences([seq[0] for seq in test_sequences], maxlen=max_len, padding='post')\n",
    "y_test_nn = pad_sequences([seq[1] for seq in test_sequences], maxlen=max_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tags to one-hot encoding\n",
    "# We need to one-hot encode the target sequences.\n",
    "# The output layer predicts the probability distribution over tags for each word in the sequence.\n",
    "# The shape should be (num_samples, max_len, num_tags_nn)\n",
    "y_train_nn_one_hot = to_categorical(y_train_nn, num_classes=num_tags_nn)\n",
    "y_val_nn_one_hot = to_categorical(y_val_nn, num_classes=num_tags_nn)\n",
    "y_test_nn_one_hot = to_categorical(y_test_nn, num_classes=num_tags_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Neural Network Model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(input_dim=vocab_size_nn, output_dim=embedding_dim, input_length=max_len))\n",
    "\n",
    "# LSTM layers\n",
    "# Using Bidirectional LSTM can often capture context from both directions\n",
    "model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(lstm_units, return_sequences=True))) # return_sequences=True to output sequence for the next layer\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(dense_units, activation='relu'))\n",
    "model.add(Dropout(0.5)) # Add dropout for regularization\n",
    "model.add(Dense(dense_units, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(dense_units, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "# For each word in the sequence, we want to predict the probability distribution over tags\n",
    "# Shape: (batch_size, max_len, num_tags_nn)\n",
    "model.add(Dense(num_tags_nn, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# Using Adam optimizer and categorical crossentropy loss for multi-class classification\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']) # Accuracy is a good metric for this task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "print(\"\\nTraining Neural Network POS Tagger...\")\n",
    "history = model.fit(X_train_nn, y_train_nn_one_hot,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_val_nn, y_val_nn_one_hot))\n",
    "print(\"Neural Network POS Tagger training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"\\nEvaluating Neural Network POS Tagger on test set...\")\n",
    "loss, accuracy = model.evaluate(X_test_nn, y_test_nn_one_hot, batch_size=batch_size)\n",
    "print(f\"Neural Network POS Tagger Test Loss: {loss:.4f}\")\n",
    "print(f\"Neural Network POS Tagger Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note on accuracy: The Keras accuracy calculates the percentage of words\n",
    "# for which the predicted tag matches the true tag, including padded positions.\n",
    "# A more accurate evaluation would ignore padding.\n",
    "\n",
    "def evaluate_nn_accuracy(model, X, y_true, index_to_tag):\n",
    "    \"\"\"\n",
    "    Evaluates the neural network model accuracy, ignoring padding.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_tags_indices = np.argmax(y_pred, axis=-1)\n",
    "    y_true_tags_indices = np.argmax(y_true, axis=-1)\n",
    "\n",
    "    correct_tags = 0\n",
    "    total_tags = 0\n",
    "\n",
    "    for true_seq, pred_seq in zip(y_true_tags_indices, y_pred_tags_indices):\n",
    "        for true_tag_idx, pred_tag_idx in zip(true_seq, pred_seq):\n",
    "            # Check if the true tag is NOT the padding tag index\n",
    "            if index_to_tag.get(true_tag_idx, '<UNK>') != '<PAD>':\n",
    "                if true_tag_idx == pred_tag_idx:\n",
    "                    correct_tags += 1\n",
    "                total_tags += 1\n",
    "\n",
    "    accuracy = correct_tags / total_tags if total_tags > 0 else 0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with custom function ignoring padding\n",
    "nn_accuracy_no_padding = evaluate_nn_accuracy(model, X_test_nn, y_test_nn_one_hot, index_to_tag)\n",
    "print(f\"Neural Network POS Tagger Test Accuracy (ignoring padding): {nn_accuracy_no_padding:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the Neural Network model\n",
    "# Example prediction on a sample test sentence\n",
    "sample_test_X_nn = X_test_nn[0:1] # Predict for the first test sentence (as a batch of 1)\n",
    "sample_test_true_tags_nn = [index_to_tag.get(idx, '<UNK>') for idx in y_test_nn[0] if index_to_tag.get(idx, '<UNK>') != '<PAD>']\n",
    "\n",
    "predicted_probs_nn = model.predict(sample_test_X_nn)\n",
    "predicted_tags_indices_nn = np.argmax(predicted_probs_nn, axis=-1) # Get the index with the highest probability for each word\n",
    "predicted_tags_nn = [index_to_tag.get(idx, '<UNK>') for idx in predicted_tags_indices_nn[0]] # Get the tag strings for the first sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out padding from predicted tags based on true tags (for comparison)\n",
    "predicted_tags_nn_filtered = [pred_tag for pred_tag, true_tag_idx in zip(predicted_tags_nn, y_test_nn[0]) if index_to_tag.get(true_tag_idx, '<UNK>') != '<PAD>']\n",
    "sample_test_words_filtered = [index_to_word.get(idx, '<UNK>') for idx in X_test_nn[0] if index_to_tag.get(y_test_nn[0][np.where(X_test_nn[0] == idx)[0][0]], '<UNK>') != '<PAD>']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNeural Network POS Tagger Prediction Example:\")\n",
    "print(\"Sentence (first test sentence):\", sample_test_words_filtered)\n",
    "print(\"True Tags:\", sample_test_true_tags_nn)\n",
    "print(\"Predicted Tags (NN):\", predicted_tags_nn_filtered)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
